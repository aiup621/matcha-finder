import re
from dotenv import load_dotenv
from search_google import search_candidates
from crawl_site import fetch_site_safe as fetch_site
from verify_matcha import verify_matcha
from rules import is_independent_strict as is_independent, normalize_url, homepage_of, extract_brand_name, instagram_handle, is_recent_enough
from sheet_io import open_sheet, append_rows_batched, get_existing_official_urls

US_STATES = ["California", "Washington", "New York", "Texas", "Oregon", "Illinois"]

def is_http_ok(status):
    try:
        return int(status) == 200
    except Exception:
        return False

EXCLUDE_PATTERNS = r"(no-?reply|career|jobs|recruit|press|media|reservation|booking|order|orders)"
PRIO1 = r"(owner|founder|ceo|gm|manager|mgr|buyer)"
PRIO2 = r"(info|hello|contact)"
PRIO3 = r"(catering|events)"

def pick_best_email(emails):
    if not emails:
        return ""
    cand = [e.lower() for e in emails if not re.search(EXCLUDE_PATTERNS, e.lower())]
    for p in (PRIO1, PRIO2, PRIO3):
        for e in cand:
            if re.search(p, e):
                return e
    return cand[0] if cand else ""

def main():
    load_dotenv()
    ws = open_sheet()
    existing_urls = get_existing_official_urls(ws)
    newly_added_urls = set()
    skipped_dupe = 0
    out_rows = []

    for state in US_STATES:
        q = f'"matcha latte" cafe {state} menu'
        for src_url in search_candidates(q, num=10):
            home = homepage_of(src_url)

            site_src  = fetch_site(src_url,  screenshot_path=None)
            site_home = fetch_site(home,     screenshot_path=None)

            if not is_http_ok(site_home.get("status")):
                continue

            insta = site_home.get("instagram") or site_src.get("instagram")
            if not insta:
                continue

            norm_home = normalize_url(home)
            if norm_home in existing_urls or norm_home in newly_added_urls:
                skipped_dupe += 1
                continue

            menu_urls = (site_home.get("menus") or []) + (site_src.get("menus") or [])
            how, evidence = verify_matcha(menu_urls, insta, site_home.get("html"))
            if not how:
                continue

            if not is_independent(site_home.get("html")):
                continue

            name = (extract_brand_name(site_home.get("html"), home) or "店名不明")[:60]
            country = "USA"

            best_email = pick_best_email(site_home.get("emails") or []) or \
                         pick_best_email(site_src.get("emails") or [])
            form_url = (site_home.get("forms")[0] if site_home.get("forms") else
                        (site_src.get("forms")[0] if site_src.get("forms") else ""))

            row = [name, country, home, insta, best_email, form_url]
            out_rows.append(row)
            newly_added_urls.add(norm_home)

    if out_rows:
        append_rows_batched(ws, out_rows)
        print(f"added {len(out_rows)} rows, skipped {skipped_dupe} duplicates")
    else:
        print(f"no new rows, skipped {skipped_dupe} duplicates")

if __name__ == "__main__":
    main()

TARGET_NEW =  10




