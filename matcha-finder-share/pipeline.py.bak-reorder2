import os, re, itertools
from dotenv import load_dotenv
from search_google import search_candidates_iter
from crawl_site import fetch_site_safe as fetch_site
from verify_matcha import verify_matcha
from rules import (
    is_independent_strict as is_independent,
    normalize_url, homepage_of, extract_brand_name,
    instagram_handle, is_recent_enough, is_delivery_or_portal
)
from sheet_io import open_sheet, append_rows_batched, get_existing_official_urls

# ---- 設定 ----
TARGET_NEW = int(os.getenv("TARGET_NEW", "100"))       # 目標件数（envで上書き可）
FRESH_DAYS = int(os.getenv("FRESH_WITHIN_DAYS", "730"))# 新しさ（日）
SEARCH_NUM = 10                                        # 1ページあたり件数
START_STEPS = [1, 11, 21, 31, 41, 51, 61, 71, 81]      # CSEのページ開始位置（必要なら追加）

# 検索対象（まず広めの州集合、あとで語彙拡張）
STATES = [
 "California","Washington","New York","Texas","Oregon","Illinois","Florida","Colorado","Massachusetts",
 "Arizona","Pennsylvania","Nevada","Georgia","Hawaii","Virginia","New Jersey","Minnesota","Maryland",
 "North Carolina","Ohio","Michigan","Tennessee","Utah","Wisconsin","Missouri","Indiana","Connecticut",
 "District of Columbia","Rhode Island","New Mexico","Oklahoma","Iowa","Idaho","Kansas","Kentucky",
 "Louisiana","Maine","Montana","Nebraska","New Hampshire","North Dakota","South Carolina","South Dakota",
 "Vermont","West Virginia","Wyoming","Alabama","Arkansas","Delaware","Mississippi","Alaska"
]

# 段階的に語彙を拡張（ラウンドが進むほど広く・曖昧に）
QUERY_TEMPLATES_ROUNDS = [
    ['"matcha latte" cafe {state} menu'],
    ['"matcha latte" cafe {state}', 'matcha cafe {state} menu', 'matcha latte {state} menu'],
    ['"matcha latte" cafe menu', 'matcha cafe "menu"'],
    ['抹茶 ラテ カフェ {state}', '抹茶 カフェ {state} メニュー'],
    ['matcha "menu" {state}', 'matcha latte "menu"', 'matcha dessert cafe {state}'],
]

EXCLUDE_PATTERNS = r"(no-?reply|career|jobs|recruit|press|media|reservation|booking|order|orders)"
PRIO1 = r"(owner|founder|ceo|gm|manager|mgr|buyer)"
PRIO2 = r"(info|hello|contact)"
PRIO3 = r"(catering|events)"

def is_http_ok(status):
    try: return int(status) == 200
    except: return False

def pick_best_email(emails):
    if not emails: return ""
    cand = [e.lower() for e in emails if not re.search(EXCLUDE_PATTERNS, e.lower())]
    for p in (PRIO1, PRIO2, PRIO3):
        for e in cand:
            if re.search(p, e): return e
    return cand[0] if cand else ""

def handle_candidate(src_url, existing_urls, newly_added_urls, out_rows):
    """1候補を評価して、要件を満たせば out_rows に追記。成功なら True"""
    home = homepage_of(src_url)
    if is_delivery_or_portal(home):
        return False

    norm_home = normalize_url(home)
    if norm_home in existing_urls or norm_home in newly_added_urls:
        return False

    # まずトップだけ取得（高速）
    site_home = fetch_site(home, screenshot_path=None)
    if not is_http_ok(site_home.get("status")):
        return False

    # IG（トップに無ければ src 側を掘る）
    insta = site_home.get("instagram")
    site_src = None
    if not insta:
        site_src = fetch_site(src_url, screenshot_path=None)
        insta = site_src.get("instagram")

    if not insta:   # IGは必須
        return False

    # メニューURL（トップ→足りなければsrc）
    menu_urls = (site_home.get("menus") or [])
    if not menu_urls and site_src:
        menu_urls += (site_src.get("menus") or [])

    # 新しさ（まずトップ/メニューだけ）
    if not is_recent_enough(home, site_home.get("html") or "", insta_html="", menu_urls=menu_urls, days=FRESH_DAYS):
        # だめならIG HTMLを初めて取りに行く
        insta_html = ""
        try:
            site_ig = fetch_site(insta, screenshot_path=None)
            insta_html = site_ig.get("html") or ""
        except:
            pass
        if not is_recent_enough(home, site_home.get("html") or "", insta_html=insta_html, menu_urls=menu_urls, days=FRESH_DAYS):
            return False

    # 抹茶提供の確認
    how, evidence = verify_matcha(menu_urls, insta, site_home.get("html"))
    if not how:
        return False

    # 独立店（チェーン除外）
    if not is_independent(site_home.get("html")):
        return False

    # 出力
    name = (extract_brand_name(site_home.get("html"), home) or "店名不明")[:60]
    country = "USA"
    best_email = pick_best_email(site_home.get("emails") or []) or \
                 pick_best_email((site_src.get("emails") or []) if site_src else [])
    form_url = (site_home.get("forms")[0] if site_home.get("forms") else
                (site_src.get("forms")[0] if (site_src and site_src.get("forms")) else ""))

    out_rows.append([name, country, home, best_email, form_url, insta])
    newly_added_urls.add(norm_home)
    return True

def main():
    load_dotenv()
    ws = open_sheet()
    existing_urls = get_existing_official_urls(ws)
    newly_added_urls = set()
    out_rows, skipped_dupe = [], 0
    pbar = tqdm(total=TARGET_NEW, desc="追加進捗", unit="件", ncols=80)

    seen_query_pages = set()  # 同じ (query,start) の再実行を避ける
    round_idx = 0

    # ====== ここから「指定件数に達するまで」無限拡大 ======
    while len(out_rows) < TARGET_NEW:
        # ラウンドごとに語彙テンプレートを切り替え（広げる）
        templates = QUERY_TEMPLATES_ROUNDS[min(round_idx, len(QUERY_TEMPLATES_ROUNDS)-1)]

        # 州あり→州なしの順に検索幅を広げる
        state_sets = [STATES, [""]]  # 最後は州なし
        progress_this_round = 0

        for states in state_sets:
            for state in states:
                for tmpl in templates:
                    q = tmpl.format(state=state).strip()
                    for start in START_STEPS:
                        print(f"[探索] {q} | start={start}")
                        key = f"{q}@@{start}"
                        if key in seen_query_pages:
                            continue
                        seen_query_pages.add(key)

                        # 1ページ分（10件）取得
                        for src_url in search_candidates_iter(q, num=SEARCH_NUM, start=start, max_pages=1):
                            # 既出チェック用にホームURLだけ先に正規化
                            home_norm = normalize_url(homepage_of(src_url))
                            if home_norm in existing_urls or home_norm in newly_added_urls:
                                skipped_dupe += 1
                                continue

                            ok = handle_candidate(src_url, existing_urls, newly_added_urls, out_rows)
                            if ok:
                                pbar.update(1)
                                print(f"✓ 追加 {len(out_rows)}/{TARGET_NEW}")
                                progress_this_round += 1
                                # 目標に達したら即、書き込み＆終了
                                if len(out_rows) >= TARGET_NEW:
                                    pbar.close()
                                    append_rows_batched(ws, out_rows)
                                    print(f"added {len(out_rows)} rows, skipped {skipped_dupe} duplicates")
                                    return
        # ラウンドで全く進捗が無ければ、語彙をさらに広げる（次ラウンドへ）
        # ただし、外部ソースが枯渇している可能性もある点はログで明示
        if progress_this_round == 0:
            # これ以上広げても収穫ゼロの兆候 → それでも続ける（次ラウンドへ）
            pass

        round_idx += 1

    # ループを抜けた場合（通常は return 済み）
    if out_rows:
        pbar.close()
        append_rows_batched(ws, out_rows)
        print(f"added {len(out_rows)} rows, skipped {skipped_dupe} duplicates")
    else:
        print(f"no new rows, skipped {skipped_dupe} duplicates")
if __name__ == "__main__":
    main()


