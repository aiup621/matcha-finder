# crawl_site.py
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

FORM_HINTS = ["contact","inquiry","support","connect","get-in-touch","customer","contact-us"]
MENU_HINTS = ["menu","drink","beverage","pdf"]

def _abs(base, href):
    return href if href.startswith("http") else urljoin(base, href)

def fetch_site(url, screenshot_path=None):
    with sync_playwright() as p:
        b = p.chromium.launch()
        page = b.new_page()
        resp = page.goto(url, wait_until="domcontentloaded", timeout=30000)
        status = resp.status if resp else None
        html = page.content()
        soup = BeautifulSoup(html, "lxml")

        emails, forms, insta, menus = set(), [], None, []
        for a in soup.select("a[href]"):
            href = a["href"]; low = href.lower()
            if low.startswith("mailto:"):
                emails.add(href.replace("mailto:", "").strip())
            if any(h in low for h in FORM_HINTS):
                forms.append(_abs(page.url, href))
            if "instagram.com" in low and not insta:
                insta = _abs(page.url, href)
            if any(w in low for w in MENU_HINTS):
                menus.append(_abs(page.url, href))

        title = (soup.title.string or "").strip() if soup.title else ""
        if screenshot_path:
            page.screenshot(path=screenshot_path, full_page=True)
        b.close()

        def uniq(seq):
            s=set(); out=[]
            for x in seq:
                if x not in s: s.add(x); out.append(x)
            return out

        return {"status": status, "html": html, "title": title,
                "emails": list(emails), "forms": uniq(forms)[:3],
                "instagram": insta, "menus": uniq(menus)[:5], "final_url": url}
