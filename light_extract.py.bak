import re, io, os, json, binascii
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlsplit, urlunsplit
from pdfminer.high_level import extract_text as pdf_extract_text

HDRS = {"User-Agent": "Mozilla/5.0 matcha-finder-bot"}
TIMEOUT = float(os.getenv("HTTP_TIMEOUT", "10"))

BLOCK_DOMAINS = {
    "yelp.com","m.yelp.com","ubereats.com","doordash.com","tripadvisor.com","opentable.com",
    "facebook.com","m.facebook.com","tiktok.com","grubhub.com","seamless.com",
    "resy.com","sevenrooms.com","toasttab.com","square.site","foodbooking.com",
    "pinterest.com","reddit.com","medium.com","substack.com","wordpress.com","blogspot.com",
    "eater.com","la.eater.com","theinfatuation.com","vogue.com","nytimes.com","forbes.com",
    "instagram.com","www.instagram.com"
}

# 既知チェーン（今回の目的外）
NON_CAFE_OR_CHAIN = {
    "alfred.la","order.alfred.la","lalalandkindcafe.com","chachamatcha.com","parisbaguette.com",
    "bluestonelane.com","blujamcafe.com","milliescafela.com","hasakinyc.com","atlaskitchennyc.com",
    "kettl.co","nanasgreenteaus.com","sunrisemart.com","originalcakenyc.com",
}

ORDER_SUB_RE = re.compile(r'^(order|orders|onlineordering|orderonline)\.', re.I)
ORDER_PATH_HINTS = ("/order","/order/","/online-order","/order-online","/order_now","/order-now","/orderonline")

US_STATE_ABBR = {
    "AL","AK","AZ","AR","CA","CO","CT","DE","FL","GA","HI","ID","IL","IN","IA","KS","KY","LA","ME","MD",
    "MA","MI","MN","MS","MO","MT","NE","NV","NH","NJ","NM","NY","NC","ND","OH","OK","OR","PA","RI","SC",
    "SD","TN","TX","UT","VT","VA","WA","WV","WI","WY","DC"
}
US_ZIP_RE   = re.compile(r"\b\d{5}(?:-\d{4})?\b")
US_PHONE_RE = re.compile(r"(?:\+1[\s\-.]?)?(?:\(?\d{3}\)?[\s\-.]?\d{3}[\s\-.]?\d{4})\b")
EMAIL_RE    = re.compile(r"[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}", re.I)

def _host(u: str) -> str:
    try:
        h = urlsplit(u).netloc.lower()
        return h[4:] if h.startswith("www.") else h
    except Exception:
        return u

def canon_root(u: str) -> str:
    try:
        s = urlsplit(u); scheme="https"
        netloc=(s.netloc or "").lower()
        if netloc.startswith("www."): netloc=netloc[4:]
        return urlunsplit((scheme, netloc, "/", "", ""))
    except Exception:
        return u

def canon_url(u: str) -> str:
    if not u: return ""
    try:
        s = urlsplit(u)
        scheme = "https" if s.scheme in ("http","https","") else s.scheme
        netloc = (s.netloc or "").lower()
        if netloc.startswith("www."): netloc = netloc[4:]
        path = s.path or "/"
        if not path.endswith("/"): path += "/"
        return urlunsplit((scheme, netloc, path, "", ""))
    except Exception:
        return u

def normalize_candidate_url(u: str) -> str:
    """order.* や /order… はルートへ戻す"""
    try:
        s = urlsplit(u)
        host = _host(u)
        labels = host.split(".")
        if labels and ORDER_SUB_RE.match(labels[0]):
            host = ".".join(labels[1:])
        if any(h in (s.path or "").lower() for h in ORDER_PATH_HINTS):
            return "https://" + host + "/"
        return u
    except Exception:
        return u

def is_blocked(u: str) -> bool:
    try:
        host = _host(u)
        if ORDER_SUB_RE.match(host.split(".")[0]): return True
        if any(host==d or host.endswith("."+d) for d in (BLOCK_DOMAINS|NON_CAFE_OR_CHAIN)):
            return True
        return False
    except Exception:
        return False

def http_get(u: str, timeout=TIMEOUT):
    try:
        r = requests.get(u, headers=HDRS, timeout=timeout, allow_redirects=True)
        if r.status_code >= 400:
            return None
        return r
    except requests.RequestException:
        return None

def html_text(html: str) -> str:
    soup = BeautifulSoup(html, "lxml")
    for t in soup(["script","style","noscript"]): t.decompose()
    return " ".join(soup.get_text(" ", strip=True).split())

MATCHA_MENU_RE = re.compile(r"(matcha|抹茶|matcha\s*latte|抹茶ラテ|green\s*tea\s*latte|ceremonial\s*matcha|iced\s*matcha|dirty\s*matcha)", re.I)
def has_matcha_text(html: str) -> bool:
    return bool(MATCHA_MENU_RE.search(html_text(html)))

MENU_HINT_RE = re.compile(r"menu|drink|beverage|tea|抹茶|メニュー|ドリンク", re.I)
def find_menu_links(html: str, base: str, limit=6):
    soup = BeautifulSoup(html, "lxml")
    links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]; text = (a.get_text() or "")
        low = href.lower()
        if ("/menu" in low) or ("drinks" in low) or MENU_HINT_RE.search(text):
            links.append(urljoin(base, href))
    for p in ["/menu","/menus","/our-menu","/drinks","/beverage","/tea","/drink-menu","/drinksmenu","/cafe-menu"]:
        links.append(urljoin(base, p))
    seen, out = set(), []
    for u in links:
        if u not in seen:
            seen.add(u); out.append(u)
    return out[:limit]

# --- 連絡先抽出：obfuscation・JSON-LD・外部フォームにも対応 ---
def _decode_cfemail(s: str) -> str:
    try:
        data = bytes.fromhex(s)
        key = data[0]
        return "".join(chr(b ^ key) for b in data[1:])
    except Exception:
        return ""

def _deobf_text_emails(t: str):
    t = re.sub(r"\s*(?:\(|\[)?at(?:\)|\])\s*", "@", t, flags=re.I)
    t = re.sub(r"\s*(?:\(|\[)?dot(?:\)|\])\s*", ".", t, flags=re.I)
    t = t.replace("（at）","@").replace("（dot）",".")
    return EMAIL_RE.findall(t)

def _jsonld_contacts(html: str):
    ig=""; emails=set()
    soup = BeautifulSoup(html, "lxml")
    for s in soup.find_all("script", attrs={"type":"application/ld+json"}):
        try:
            data = json.loads(s.string or "")
        except Exception:
            continue
        def walk(x):
            nonlocal ig, emails
            if isinstance(x, dict):
                if "sameAs" in x:
                    vals = x["sameAs"]
                    if isinstance(vals, str): vals=[vals]
                    for v in vals or []:
                        if isinstance(v, str) and "instagram.com" in v.lower():
                            ig = canon_url(v)
                if "email" in x and isinstance(x["email"], str):
                    emails.add(x["email"])
                if "contactPoint" in x and isinstance(x["contactPoint"], list):
                    for cp in x["contactPoint"]:
                        if isinstance(cp, dict) and isinstance(cp.get("email"), str):
                            emails.add(cp["email"])
                for v in x.values(): walk(v)
            elif isinstance(x, list):
                for v in x: walk(v)
        walk(data)
    return ig, sorted(emails)

def find_contacts_light(base: str, html: str):
    ig = ""; emails = set(); form = ""
    soup = BeautifulSoup(html, "lxml")

    # JSON-LD
    ig0, em0 = _jsonld_contacts(html)
    if ig0: ig = ig0
    for e in em0: emails.add(e)

    # aタグから抽出
    for a in soup.find_all("a", href=True):
        href = a["href"]; low = href.lower()
        if "instagram.com" in low and not ig:
            ig = canon_url(urljoin(base, href))
        if low.startswith("mailto:"):
            emails.add(low.replace("mailto:","").strip())
        if any(k in low for k in ["/contact","/inquiry","/contact-us","/contactus","/get-in-touch","/support"]):
            cand = urljoin(base, href)
            rr = http_get(cand)
            if rr and rr.text:
                # Cloudflare obfuscation
                s2 = BeautifulSoup(rr.text, "lxml")
                for cf in s2.select('a[data-cfemail], span.__cf_email__'):
                    enc = cf.get("data-cfemail")
                    if enc:
                        dec = _decode_cfemail(enc)
                        if dec: emails.add(dec)
                # テキスト obfuscation
                for m in _deobf_text_emails(s2.get_text(" ", strip=True)):
                    emails.add(dec)
                # 通常メール
                for m in EMAIL_RE.findall(rr.text):
                    emails.add(dec)
                # 外部フォーム
                for a2 in s2.find_all("a", href=True):
                    l2 = a2["href"].lower()
                    if any(p in l2 for p in ["typeform.com","jotform.com","formspree.io","forms.gle","docs.google.com/forms","wufoo.com"]):
                        form = urljoin(base, a2["href"])

    # ルート直下の定番ページも試す
    for p in ["/contact","/contact-us","/about","/privacy","/visit","/find-us","/locations","/location","/hours","/support"]:
        rr = http_get(urljoin(base, p))
        if rr and rr.text:
            s2 = BeautifulSoup(rr.text, "lxml")
            if not form and "<form" in rr.text.lower():
                form = urljoin(base, p)
            for cf in s2.select('a[data-cfemail], span.__cf_email__'):
                enc = cf.get("data-cfemail")
                if enc:
                    dec = _decode_cfemail(enc)
if dec:
    emails.add(dec)
            for m in _deobf_text_emails(s2.get_text(" ", strip=True)):
                emails.add(dec)
            for m in EMAIL_RE.findall(rr.text):
                emails.add(dec)
            if not ig:
                for a2 in s2.find_all("a", href=True):
                    if "instagram.com" in (a2["href"] or "").lower():
                        ig = canon_url(urljoin(base, a2["href"]))

    return (ig, sorted(emails), form)

def _ld_types(html: str):
    out=set(); soup = BeautifulSoup(html, "lxml")
    for s in soup.find_all("script", attrs={"type":"application/ld+json"}):
        try: data = json.loads(s.string or "")
        except Exception: continue
        def walk(x):
            if isinstance(x, dict):
                t = x.get("@type")
                if isinstance(t, str): out.add(t.lower())
                elif isinstance(t, list):
                    for tt in t:
                        if isinstance(tt, str): out.add(tt.lower())
                for v in x.values(): walk(v)
            elif isinstance(x, list):
                for v in x: walk(v)
        walk(data)
    return out

def is_blog_like(base: str, html: str) -> bool:
    low = html.lower()
    if 'name="generator"' in low and 'wordpress' in low: return True
    if 'wp-content/' in low or 'wp-json' in low: return True
    if '/category/' in low or '/tag/' in low or '/author/' in low: return True
    soup = BeautifulSoup(html, "lxml"); arts = soup.find_all("article")
    return len(arts) >= 3

def is_media_or_platform(base: str, html: str) -> bool:
    host = _host(base)
    if ORDER_SUB_RE.match(host.split(".")[0]): return True
    lowurl = (base or "").lower()
    if any(h in lowurl for h in ORDER_PATH_HINTS): return True
    types = _ld_types(html)
    if {"cafeorcoffeeshop","restaurant","foodestablishment","bakery","teashop"}.intersection(types):
        return False
    soup = BeautifulSoup(html, "lxml")
    if any(("/menu" in (a.get("href") or "").lower()) for a in soup.find_all("a", href=True)): return False
    if _host(base) in BLOCK_DOMAINS: return True
    if is_blog_like(base, html): return True
    low = html.lower()
    if 'property="og:type"' in low and "article" in low: return True
    return False

def is_us_cafe_site(base: str, html: str) -> bool:
    texts=[html_text(html)]
    soup = BeautifulSoup(html, "lxml")
    for p in ["/contact","/locations","/visit","/find-us","/about","/hours","/location"]:
        rr=http_get(urljoin(base,p))
        if rr and rr.text: texts.append(html_text(rr.text))
    T=" ".join(texts).lower()
    if not (US_ZIP_RE.search(T) or US_PHONE_RE.search(T) or re.search(r"\b(?:%s)\b" % "|".join(US_STATE_ABBR), T)):
        return False
    if any(w in T for w in ("cafe","coffee","tea","teahouse","bakery","boba","bubble tea","matcha")): return True
    host=_host(base)
    if any(k in host for k in ("cafe","coffee","tea")): return True
    if {"cafeorcoffeeshop","restaurant","foodestablishment","bakery","teashop"}.intersection(_ld_types(html)): return True
    return False

def guess_brand(html: str, home: str) -> str:
    soup = BeautifulSoup(html, "lxml")
    m = soup.find("meta", attrs={"property":"og:site_name"}) or soup.find("meta", attrs={"name":"og:site_name"})
    if m and m.get("content"): return m["content"].strip()
    if soup.title and soup.title.string:
        t = soup.title.string.strip()
        t = re.split(r"[-–—|•]| \| ", t)[0].strip()
        if t: return t
    return urlsplit(home).netloc.split(":")[0]

def is_chain_like(base: str, html: str) -> bool:
    loc = urljoin(base, "/locations")
    rr = http_get(loc)
    if rr and rr.text:
        soup = BeautifulSoup(rr.text, "lxml")
        cards = soup.select('[class*="location"], [class*="store"], .card, li')
        if len(cards) >= 20:  # 閾値上げ
            return True
    addr_hits = len(re.findall(r"\d{1,5}\s+\w+\s+\w+|[A-Za-z]+,\s*[A-Z]{2}\s*\d{5}", html))
    return addr_hits >= 30
